{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Project\n",
    "\n",
    "In this notebook, I explore the data provided for question #1 and #2, and demonstrate my data exploration methods and reasoning to identify potential markets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Import Libraries and Set Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altair is a visualization tool similar to Matplotlib, Seaborn, or other tools. Altair has beautiful themese that you can use, so here I selected the 'fiverthirtyeight' theme, which replicates the visual style that fivethirtyeight typically uses.\n",
    "# Altair also imposes a limit on 4,000 rows of data by default, so I set alt.data_transformers.disable_max_rows() to display certain visualizations.\n",
    "alt.themes.enable('fivethirtyeight')\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the PLD for Q1\n",
    "q1_df = pd.read_csv('data/shipments/pld_table/202302 BIE Technical Interview PLD1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial look at the data\n",
    "q1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the shape of the data\n",
    "q1_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view datatypes\n",
    "q1_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert origin and destination zip codes to string type so Altair recognizes it as categorical instead of numerical. This can sometimes effect visualizations, and zip codes should be categorical data anyway.\n",
    "q1_df['OriginZip'] = q1_df['OriginZip'].astype(str)\n",
    "q1_df['DestinationZip'] = q1_df['DestinationZip'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like leading 0s were dropped from zip codes at some point in exporting or loading the csv, let's fill in these leading 0s. Some are missing 2 leadings 0s and others only one.\n",
    "\n",
    "print(len(q1_df[q1_df['DestinationZip'].str.len() == 3]), len(q1_df[q1_df['DestinationZip'].str.len() == 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and appl function to fill in leading 0s\n",
    "def fill_leading_zeros(x):\n",
    "    x = x.zfill(5)\n",
    "    return x\n",
    "\n",
    "q1_df['OriginZip'] = q1_df['OriginZip'].apply(fill_leading_zeros)\n",
    "q1_df['DestinationZip'] = q1_df['DestinationZip'].apply(fill_leading_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any zip codes are missing leading 0s again\n",
    "print(len(q1_df[q1_df['DestinationZip'].str.len() == 3]), len(q1_df[q1_df['DestinationZip'].str.len() == 4]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the distribution of origin zip codes\n",
    "origin_zips = q1_df['OriginZip'].value_counts(ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like origin zip codes are heavily skewed, a lot of volume is originating from the same place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will plot an altair bar chart to show the distribution of origin zip codes. The visualization includes a tooltip that you can hover over to view the zip code and count for each card. You can also pass in the original dataframe if you want to view more tooltips such as destination zip, total order count, average package weight, etc.\n",
    "alt.Chart(origin_zips).mark_bar().encode(\n",
    "    x=alt.X('OriginZip:N', axis=alt.Axis(title='Origin Zip Code'), sort=None),\n",
    "    y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n",
    "    tooltip=['OriginZip', 'count']\n",
    ").configure_axisX(\n",
    "    labels=False\n",
    ").properties(\n",
    "    width=2500,\n",
    "    height=500,\n",
    "    title='Distribution of Origin Zip Codes'\n",
    ").configure_title(\n",
    "    anchor='middle'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the distribution of destination zip codes.\n",
    "destination_zips = q1_df['DestinationZip'].value_counts(ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination zip codes appear to follow a similar distirbution to origin zip codes, although there are significantly more destinations than origins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of destination zip codes with tool tips. Since there are so many data points, it is difficult to see. This could be improved by filtering for a specific origin zip code.\n",
    "alt.Chart(destination_zips).mark_bar().encode(\n",
    "    x=alt.X('DestinationZip:N', axis=alt.Axis(title='Destination Zip Code'), sort=None),\n",
    "    y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n",
    "    tooltip=['DestinationZip', 'count']\n",
    ").configure_axisX(\n",
    "    labels=False\n",
    ").properties(\n",
    "    width=2500,\n",
    "    height=500,\n",
    "    title='Distribution of Destination Zip Codes'\n",
    ").configure_title(\n",
    "    anchor='middle'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by origin an destination zip and take sum of total packages for a specific route\n",
    "grouped_q1_df = q1_df.groupby(by=['OriginZip', 'DestinationZip']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_q1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like there are 323,938 distinct routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will create a concatenated column which contains the origin and destination zip codes combined, which makes it easier to plot a specific \"route\".\n",
    "grouped_q1_df['Route'] = 'Origin: ' + grouped_q1_df['OriginZip'] + '- Destination: ' + grouped_q1_df['DestinationZip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this is for September, 2022, just create a variable for # of days. This could be dynamic if we had more months, or likely would already have a feature denoting the month.\n",
    "days_in_september = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average packages per day for a given route through the month of September.\n",
    "grouped_q1_df['AveragePerDay'] = (grouped_q1_df['OrderCount'] / days_in_september).round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the new dataframe in descending order by average packages per day.\n",
    "grouped_q1_df.sort_values('AveragePerDay',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average is rounded to the nearest number, some routes averaged less than 1 per day, and were therefore rounded down to 0. If a client wanted, we could represent this in a different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While the question asks about average packages per day, it is still important to view the distribution of the total number of orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_counts = grouped_q1_df.sort_values('OrderCount', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(order_counts).mark_bar().encode(\n",
    "    x=alt.X('Route:N', axis=alt.Axis(title='Route'), sort=None),\n",
    "    y=alt.Y('OrderCount:Q', axis=alt.Axis(title='Order Count')),\n",
    "    tooltip=['Route:N', 'OrderCount']\n",
    ").configure_axisX(\n",
    "    labels=False\n",
    ").properties(\n",
    "    width=1500,\n",
    "    height=200,\n",
    "    title='Distribution of Zip Code Routes by Order Count'\n",
    ").configure_title(\n",
    "    anchor='middle'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_order_counts = grouped_q1_df.sort_values('AveragePerDay', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we see that some routes have an average of 0 packages per day. The average is rounded to the nearest whole number, but this can be updated if it is misleading.\n",
    "average_order_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will make a histogram, which is better to represent numerical data with a lot of values. The downside of the histogram is that you can't easily view tooltips.\n",
    "alt.Chart(average_order_counts).mark_bar().encode(\n",
    "    x=alt.X('AveragePerDay:Q', axis=alt.Axis(title='Average Order Count Per Day - Binned'), bin=True),\n",
    "    y=alt.Y('count()', axis=alt.Axis(title='Number of Routes')),\n",
    ").properties(\n",
    "    width=1500,\n",
    "    height=800,\n",
    "    title='Distribution of Zip Code Routes by Average Daily Orders'\n",
    ").configure_title(\n",
    "    anchor='middle'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution is heavily skewed. Most direct routes appear to have very little volume, although a route would have to handle at least 15 packages a month to reach an average of 1 per day.\n",
    "# There are so many routes with an average of 0 per day, that the routes with a higher average are difficult to see. This makes this particular visualization misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the top route for a sanity check. It looks like one particular day they shipped 394 orders. Is there significance behind this day?\n",
    "q1_df[(q1_df['OriginZip'] == '36039') & (q1_df['DestinationZip'] == '71322')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.) Question #2: Identify Potential Markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we know what the volume looks like for given routes, let's look at destination zip codes again and get the total order count for a given destination\n",
    "destination_monthly_total = grouped_q1_df.groupby('DestinationZip')['OrderCount'].sum().reset_index()\n",
    "destination_monthly_total.sort_values('OrderCount', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to potential facilities in geographical areas where a large volume of packages are sent. In order to do this, we will use the sklearn DBSCAN algorithm, which stands for Density-Based Spatial Clustering Applications with Noise. \n",
    "# This finds clusters with high density and expands from there:  https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "# In order to do this, we need to find the latitude and longitude coordinates of zip codes in the US. For this, I used this publicly available file found here https://www2.census.gov/geo/docs/maps-data/data/gazetteer/2020_Gazetteer/2020_Gaz_zcta_national.zip\n",
    "# I am not able to verify if these are accurate, but even general accuracy should be good enough to form proper geographical clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in geographical data from the file\n",
    "zip_code_df = pd.read_csv('data/2020_Gaz_zcta_national.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEO ID = zip code, and we are interested in INTPTLAT and INTPTLONG for the latitude and longtidue coordinates\n",
    "zip_code_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since these are also missing leadings 0s, let's apply them here\n",
    "zip_code_df['DestinationZip'] = zip_code_df['GEOID'].astype(str).apply(fill_leading_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_code_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the monthly total count \n",
    "merged_df = pd.merge(destination_monthly_total, zip_code_df, on='DestinationZip', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# view the columns. It looks like there's space characters after INTPTLONG, so let's replace those\n",
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.rename(columns=lambda x: x.replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the shape of the merged df\n",
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's confirm if every zip code was matched\n",
    "merged_df[merged_df['GEOID'].isna()].sort_values('OrderCount', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it looks like the provided zip code coordinates dataset may be incomplete.  It looks like we will lose about 4.627 rows, some of which have significant volume, which could greatly impact the results. \n",
    "# Neverthless, we will drop the NA values for the sake of this exercise and make a note to find a more accurate zip code geographical coordinate source in the future.\n",
    "merged_df = merged_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an input for DBSCAN\n",
    "coords = merged_df[['INTPTLAT', 'INTPTLONG']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dbscan hyperparameters eps=.5, min_samples=100. DBSCAN is highlysensitive to these values. These can be altered based on customer needs, as it will change the total number of clusters the algorithm identifies. These parameters create 8 clusters, which represent different geographical regions of the US.\n",
    "dbscan_model = DBSCAN(eps=.5, min_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dbscan_model.fit_predict(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick view o the clusters and the total number of orders within them.\n",
    "merged_df.groupby('Cluster').sum().sort_values('OrderCount', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clusters: -1 contains the noise points which the algorithm did not attribute to a cluster. In some cases, you want to remove these noise points when plotting, but as you can see, they create a nice shape of the United States, which serves as a visual aid in identifying potential markets.\n",
    "alt.Chart(merged_df).mark_point(size=60).encode(\n",
    "    x=alt.X('INTPTLONG', axis=alt.Axis(title='Longitude')),\n",
    "    y=alt.Y('INTPTLAT', axis=alt.Axis(title='Latitude')),\n",
    "    color=alt.Color('Cluster:N'),\n",
    "    tooltip=['DestinationZip', 'OrderCount', 'Cluster']\n",
    ").properties(\n",
    "    height=800,\n",
    "    width=1600,\n",
    "    title='Potential Sortation Facilities via DBSCAN: eps=0.5, min_samples=100'\n",
    ").configure_legend(\n",
    "    padding=10,\n",
    "    cornerRadius=10,\n",
    "    orient='top-right'\n",
    ").interactive()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.) Question 3: Productionalizing Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the steps I would take and technologies I would use to productionalize different formats of PLDs from customers (as demosntrated in the file provided for question 3 located in /data/shipments/monthly_shipping_table//202302 BIE Technical Interview PLD2.csv), and turn them into visualizations or other tools for stakeholders to view results.\n",
    "\n",
    "1. Extract:\n",
    "    - Python offers a variety of packages for loading and processing data, which include pandas, PySpark, numpy, requests, SQLAlchemcy, and others, all of which can interact with data from different sources.\n",
    "    - pandas is a quick and easy tool for data manipulation, however, as business needs scale and as data grows, PySpark is generally the better option. PySpark offers distributed processing which can speed up large data operations and allows data to be cached in memory, which improves performance.\n",
    "    - All the examples I created above can be replicated above in PySpark on a larger scale and on multiple machines.\n",
    "    - Python to validate data inputs against a schema (see example schema for provided datasets located in schemas/schemas.json) if required, as well is programatically accept and clean differnet filetypes, like JSON, XML, csv, Excel, etc.\n",
    "    - In a traditional ELT Many of these steps are not necessary and Python can simply be used as a tool to retrieve raw data and load the raw data directly into a data warehouse like Snowflake or BigQuery. However, ELT can create higher loads and costs on the target system.\n",
    "\n",
    "2. Transform:\n",
    "    - There are a variety of tools available for this. The transformations could be done in PySpark to set up tables, version control\n",
    "    - PySpark can be used to clean, aggregate, and enrich data as needed, and loaded into the target system.\n",
    "    - Transformations are based on customer needs, and Pyspark can be version controlled.\n",
    "    - If ETL is not necessary and the company could consider ELT, then a tool that I am partial to is dbt (data build tool), which can be used as a transformation tool on top of a data warehouse like Snowflake or BigQuery. dbt is a great tool because it's simple to use, it has built in testing an documentation, and is easy to integrate.\n",
    "    - ELT vs. ETL ultimately depends on the needs of the company and the costs associated with it.\n",
    "\n",
    "3. Load:\n",
    "    - There are multiple options here depending on the needs of the business. A RDBMS might be required if there is transactional processing required to support daily operations of the organization, which then can be fed into a cloud-based data warehouse for analytical processes. (OLTP vs OLAP).\n",
    "    - RDBMS can be useful if real-time reporting is required, but data warehouses are superior for analytical reporting as they can accept data from different sources.\n",
    "    - Datawarehouses like Snowflake, BigQuery, or Redshift are extremely useful and be cost effective for analytical processing.\n",
    "\n",
    "4. Visualiaztion\n",
    "    - There are so many tools available for this, it's hard to pick one. Tableau, QlikView, Sisense, PowerBI, Looker, etc. This mostly just comes down to cost. Sisense is particularly good with embedding visualizations, but that is mostly useful for companies that offer a wide range of products and need to emebed visulizations within those products\n",
    "\n",
    "4. Schedule:\n",
    "    - Outside of just ETL, it is important to use some sort of data orchestration tool like Airflow or Argo Workflows (kubernetes) to schedule the pipeline to run at a specific time and monitor it for any issues.\n",
    "    - Monitoring can alert teams if there are issues with the pipeline like data ingestion or data validation failures.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " Databricks is another useful tool that natively includes Apache Spark, but I do not know the needs of the company to determine if Databricks is the correct solution based on existing architecture. Databricks is a unfifed data analytics platform that you can use to handle the entire ETL pipeline and orchestrate tasks (although not as comprehnsive as Airflow). It all ultimately depends on the needs of the company. The above steps are a high level view of the steps I would take, but the technologies used for those steps can be substituted if needed.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view a sample flowchart of an ETL pipeline orchestrated using Airflow in images/ETL pipeline.png"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
